import os
import json
import re
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue
from tqdm import tqdm
import logging
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SemanticLawRAG:
    """
    –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É
    """

    def __init__(self, qdrant_path: str = "./semantic_laws_db"):
        self.qdrant_path = qdrant_path
        self.model = None
        self.client = None

    def initialize_system(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")

        # üéØ –ò–°–ü–û–õ–¨–ó–£–ï–ú –ú–û–î–ï–õ–¨ –î–õ–Ø –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ü–û–ò–°–ö–ê
        self.model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

        self.client = QdrantClient(path=self.qdrant_path)
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def extract_semantic_chunks(self, law_item: dict) -> list:
        """
        –†–ê–ó–ë–ò–í–ê–ï–ú –¢–ï–ö–°–¢ –ù–ê –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –ß–ê–ù–ö–ò
        –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ!
        """
        original_text = law_item.get("text", "")
        if not original_text or len(original_text.strip()) < 10:
            return []

        # üéØ –û–°–ù–û–í–ù–ê–Ø –ò–î–ï–Ø: —Å–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        semantic_chunks = []

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
        sentences = self._split_into_sentences(original_text)

        # 2. –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã
        chunks = self._create_semantic_chunks(sentences, law_item)

        return chunks

    def _split_into_sentences(self, text: str) -> list:
        """–†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]

    def _create_semantic_chunks(self, sentences: list, law_item: dict) -> list:
        """–°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏"""
        chunks = []

        # üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø 1: –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –ø–æ —Å–º—ã—Å–ª—É
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)

            # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ —Å—Ç–∞–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º –∏–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ç–µ–º–∞
            if current_length + sentence_length > 200 or self._is_new_topic(sentence):
                if current_chunk:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                    chunk_text = " ".join(current_chunk)
                    if len(chunk_text) > 30:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                        chunks.append(self._create_chunk_data(chunk_text, law_item))

                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                    current_chunk = []
                    current_length = 0

            current_chunk.append(sentence)
            current_length += sentence_length

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            if len(chunk_text) > 30:
                chunks.append(self._create_chunk_data(chunk_text, law_item))

        # üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –ï—Å–ª–∏ –º–∞–ª–æ —á–∞–Ω–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
        if len(chunks) < 2 and sentences:
            key_phrases = self._extract_key_phrases(sentences, law_item)
            chunks.extend(key_phrases)

        return chunks

    def _is_new_topic(self, sentence: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–∞—è —Ç–µ–º–∞"""
        new_topic_indicators = [
            '—Ç–∞–∫–∂–µ', '–∫—Ä–æ–º–µ —Ç–æ–≥–æ', '–ø—Ä–∏ —ç—Ç–æ–º', '–≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è',
            '—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã', '–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏', '–Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏',
            '—Å—Ç–∞—Ç—å—è', '–ø—É–Ω–∫—Ç', '—á–∞—Å—Ç—å'
        ]

        sentence_lower = sentence.lower()
        return any(indicator in sentence_lower for indicator in new_topic_indicators)

    def _create_chunk_data(self, chunk_text: str, law_item: dict) -> dict:
        """–°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞"""
        return {
            "text": chunk_text,
            "source_type": law_item.get("source_type", ""),
            "source_title": law_item.get("source_title", ""),
            "hierarchy": law_item.get("hierarchy", []),
            "hierarchy_str": law_item.get("hierarchy_str", ""),
            "law_id": law_item.get("law_id", ""),
            "chunk_id": hashlib.md5(chunk_text.encode()).hexdigest()[:16],
            "source_url": law_item.get("source_url", ""),
            "original_id": law_item.get("id", ""),
            "is_semantic_chunk": True  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫
        }

    def _extract_key_phrases(self, sentences: list, law_item: dict) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
        key_chunks = []

        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
        if len(sentences) >= 2:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–º—ã—Å–ª
            main_idea = " ".join(sentences[:2])
            key_chunks.append(self._create_chunk_data(main_idea, law_item))

        # –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in
                   ['–æ–∑–Ω–∞—á–∞–µ—Ç', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—è–≤–ª—è–µ—Ç—Å—è', '–ø–æ–ª–Ω–æ–º–æ—á–∏—è', '–æ–±—è–∑–∞–Ω']):
                if len(sentence) > 20:
                    key_chunks.append(self._create_chunk_data(sentence, law_item))

        return key_chunks

    def rebuild_semantic_database(self, laws_folder: str):
        """
        –ü–ï–†–ï–°–¢–†–û–ô–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–• —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
        """
        print("üöÄ –ü–ï–†–ï–°–¢–†–û–ô–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–• –° –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ú –ü–û–ò–°–ö–û–ú")
        print("=" * 60)

        self.initialize_system()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        laws_data = self._load_original_data(laws_folder)
        if not laws_data:
            return

        # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
        semantic_data = []
        for law_item in tqdm(laws_data, desc="üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"):
            chunks = self.extract_semantic_chunks(law_item)
            semantic_data.extend(chunks)

        print(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(semantic_data)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        self._create_semantic_collection()

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
        points = self._generate_semantic_embeddings(semantic_data)
        self._upload_semantic_data(points)

        print("üéâ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –ë–ê–ó–ê –î–ê–ù–ù–´–• –ì–û–¢–û–í–ê!")

    def _load_original_data(self, folder_path: str) -> list:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        laws_data = []

        if not os.path.exists(folder_path):
            print(f"‚ùå –ü–∞–ø–∫–∞ {folder_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            return []

        json_files = [f for f in os.listdir(folder_path) if f.endswith(('.json', '.jsonl'))]

        for file_name in tqdm(json_files, desc="üìñ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤"):
            file_path = os.path.join(folder_path, file_name)

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            law_item = json.loads(line)
                            laws_data.append(law_item)
            except Exception as e:
                continue

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(laws_data)} –∏—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        return laws_data

    def _create_semantic_collection(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            if self.client.collection_exists("semantic_laws"):
                self.client.delete_collection("semantic_laws")

            vector_size = self.model.get_sentence_embedding_dimension()

            self.client.create_collection(
                collection_name="semantic_laws",
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE
                )
            )
            print(f"‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ (—Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤: {vector_size})")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")

    def _generate_semantic_embeddings(self, semantic_data: list):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
        texts = [item["text"] for item in semantic_data]

        print("üî¢ –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embeddings = self.model.encode(
            texts,
            show_progress_bar=True,
            batch_size=128,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            normalize_embeddings=True
        )

        points = []
        for i, (item, embedding) in enumerate(zip(semantic_data, embeddings)):
            point = PointStruct(
                id=i,
                vector=embedding.tolist(),
                payload={
                    "semantic_text": item["text"],
                    "original_text": item.get("text", ""),
                    "source_title": item.get("source_title", ""),
                    "hierarchy_str": item.get("hierarchy_str", ""),
                    "law_id": item.get("law_id", ""),
                    "is_semantic": True
                }
            )
            points.append(point)

        return points

    def _upload_semantic_data(self, points: list):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print(f"üì§ –ó–∞–≥—Ä—É–∑–∫–∞ {len(points)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤...")

        batch_size = 100
        for i in tqdm(range(0, len(points), batch_size), desc="‚¨ÜÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞"):
            batch = points[i:i + batch_size]
            self.client.upsert(
                collection_name="semantic_laws",
                points=batch
            )

    def semantic_search(self, query: str, limit: int = 10):
        """
        –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö - –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
        """
        if self.model is None:
            self.initialize_system()

        # üéØ –£–õ–£–ß–®–ï–ù–ò–ï: —Ä–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        enhanced_query = self._enhance_search_query(query)

        print(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: '{query}'")
        print(f"üéØ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{enhanced_query}'")

        query_embedding = self.model.encode([enhanced_query]).tolist()[0]

        results = self.client.search(
            collection_name="semantic_laws",
            query_vector=query_embedding,
            limit=limit
        )

        return results

    def _enhance_search_query(self, query: str) -> str:
        """–£–ª—É—á—à–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        query_lower = query.lower()

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
        enhancements = []

        # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å–∏–Ω–æ–Ω–∏–º—ã
        legal_synonyms = {
            '–ø–æ–ª–Ω–æ–º–æ—á–∏—è': ['–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è', '–ø—Ä–∞–≤–∞', '—Ñ—É–Ω–∫—Ü–∏–∏', '–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏'],
            '–≤–µ—Ä—Ö–æ–≤–Ω—ã–π —Å—É–¥': ['–≤—Å —Ä—Ñ', '–≤–µ—Ä—Ö–æ–≤–Ω—ã–π —Å—É–¥ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏'],
            '–æ–±—è–∑–∞–Ω': ['–¥–æ–ª–∂–µ–Ω', '–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏'],
            '–ø—Ä–∞–≤–∞': ['–ø–æ–ª–Ω–æ–º–æ—á–∏—è', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è'],
            '—Å—É–¥': ['—Å—É–¥—å—è', '—Å—É–¥—å–∏', '—Å—É–¥–µ–±–Ω—ã–π']
        }

        for term, synonyms in legal_synonyms.items():
            if term in query_lower:
                enhancements.extend(synonyms)

        enhanced_query = query
        if enhancements:
            enhanced_query += " " + " ".join(enhancements)

        return enhanced_query

    def test_semantic_search(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        test_queries = [
            "–ø–æ–ª–Ω–æ–º–æ—á–∏—è –≤–µ—Ä—Ö–æ–≤–Ω–æ–≥–æ —Å—É–¥–∞",
            "–ø—Ä–∞–≤–∞ —Å—É–¥–µ–π",
            "–æ–±–∂–∞–ª–æ–≤–∞–Ω–∏–µ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π",
            "—É–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ",
            "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å",
            "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è"
        ]

        print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ü–û–ò–°–ö–ê")
        print("=" * 60)

        for query in test_queries:
            print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
            results = self.semantic_search(query, limit=3)

            if results:
                for i, result in enumerate(results):
                    print(f"   {i + 1}. üìö {result.payload.get('source_title', '')}")
                    print(f"      üèõÔ∏è {result.payload.get('hierarchy_str', '')}")
                    print(f"      üìù {result.payload.get('semantic_text', '')[:100]}...")
                    print(f"      üéØ –°—Ö–æ–∂–µ—Å—Ç—å: {result.score:.4f}")
            else:
                print("   ‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")


def main():
    """
    –ó–∞–ø—É—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã
    """
    print("ü§ñ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–û–ò–°–ö–ê –ü–û –ó–ê–ö–û–ù–û–î–ê–¢–ï–õ–¨–°–¢–í–£")
    print("=" * 50)

    semantic_rag = SemanticLawRAG()

    laws_folder = "D:\HelperYoristBot\Laws"  # –í–∞—à–∞ –ø–∞–ø–∫–∞

    # üèóÔ∏è –ü–ï–†–ï–°–¢–†–û–ô–ö–ê –ë–ê–ó–´ (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–¥–∏–Ω —Ä–∞–∑)
    print("1. üèóÔ∏è –ü–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
    print("2. üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑–µ")

    choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1 –∏–ª–∏ 2): ").strip()

    if choice == "1":
        semantic_rag.rebuild_semantic_database(laws_folder)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    semantic_rag.test_semantic_search()


if __name__ == "__main__":
    main()
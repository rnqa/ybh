from dotenv import load_dotenv
import asyncio
import logging
import sys
import os

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import OpenAI

from aiogram import Bot, Dispatcher, types
from aiogram.filters import CommandStart

load_dotenv("config.env")
# ====================== –Æ–†–ò–î–ò–ß–ï–°–ö–ò–ô –ê–°–°–ò–°–¢–ï–ù–¢ ======================

class LegalAssistant:
    def __init__(self):
        self.embedding_dimension = 768
        self._init_embeddings()
        self._init_vector_db()
        self._init_llm()
        self._init_chain()

    def _init_embeddings(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-base",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
        logging.info("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    def _init_vector_db(self):
        os.makedirs("./db/ALL", exist_ok=True)

        self.db = Chroma(
            persist_directory="./db/ALL",
            embedding_function=self.embeddings,
            collection_metadata={"hnsw:space": "cosine"}
        )

        self.retriever = self.db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 8}
        )
        logging.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def _init_llm(self):
        self.llm = OpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            max_tokens=6000,
            timeout=60,
            max_retries=2
        )
        logging.info("OpenAI LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _format_docs_for_context(self, docs):
        if not docs:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."

        formatted = []
        for i, doc in enumerate(docs):
            content = doc.page_content.strip()

            source_info = ""
            if doc.metadata.get("source_title"):
                source_info += f"{doc.metadata['source_title']}"

            if doc.metadata.get("hierarchy"):
                source_info += f" ‚Äî {doc.metadata['hierarchy']}"

            formatted.append(
                f"[–î–æ–∫—É–º–µ–Ω—Ç {i + 1}: {source_info}]\n{content}"
            )

        return "\n\n" + "=" * 60 + "\n" + "\n\n".join(formatted) + "\n" + "=" * 60

    def _format_sources(self, docs):
        sources = []
        for doc in docs:
            title = doc.metadata.get("source_title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
            law_id = doc.metadata.get("law_id")
            hierarchy = doc.metadata.get("hierarchy")

            source = title
            if law_id:
                source += f" ({law_id})"
            if hierarchy:
                source += f" ‚Äî {hierarchy}"

            if source not in sources:
                sources.append(source)

        return sources

    def _init_chain(self):
        template = """
–¢—ã ‚Äî —Ü–∏—Ñ—Ä–æ–≤–æ–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤—É–∑–æ–≤ –†–§.



–î–û–ö–£–ú–ï–ù–¢–´:
{context}

–í–û–ü–†–û–°:
{question}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
- –ß–µ—Ç–∫–∏–π –∏ –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç
- –°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –∏ –Ω–æ—Ä–º—ã
- –ü–æ–Ω—è—Ç–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤
- –°—Ç—Ä–æ–≥–∏–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å
- –ë–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤ –∏ —Ñ–∞–Ω—Ç–∞–∑–∏–∏

–û–¢–í–ï–¢:
"""

        prompt = ChatPromptTemplate.from_template(template)

        self.chain = (
            RunnableParallel({
                "context": self.retriever | self._format_docs_for_context,
                "question": RunnablePassthrough()
            })
            | prompt
            | self.llm
            | StrOutputParser()
        )

        logging.info("RAG-—Ü–µ–ø–æ—á–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def ask(self, question: str) -> str:
        try:
            docs = self.retriever.invoke(question)
            sources = self._format_sources(docs)

            answer = self.chain.invoke(question)

            if sources:
                answer += "\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n"
                for i, src in enumerate(sources, 1):
                    answer += f"{i}. {src}\n"

            answer += "\n\n–û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ –†–§ –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —É—á–µ–±–Ω—ã—Ö —Ü–µ–ª–µ–π."

            return answer

        except Exception as e:
            logging.error("–û—à–∏–±–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞", exc_info=True)
            return "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."


# ====================== TELEGRAM-–ë–û–¢ ======================

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
if not TELEGRAM_TOKEN:
    raise RuntimeError("TELEGRAM_TOKEN –Ω–µ –∑–∞–¥–∞–Ω")

assistant = LegalAssistant()
bot = Bot(token=TELEGRAM_TOKEN)
dp = Dispatcher()


@dp.message(CommandStart())
async def start_cmd(message: types.Message):
    text = (
        "üëã *–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç*\n\n"
        "–Ø –ø–æ–º–æ–≥–∞—é —Ä–∞–∑–±–∏—Ä–∞—Ç—å—Å—è –≤ –Ω–æ—Ä–º–∞—Ö —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –ø—Ä–∞–≤–∞.\n\n"
        "üìå –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:\n"
        "‚Ä¢ –ö–∞–∫–∏–µ –ø–æ–ª–Ω–æ–º–æ—á–∏—è —É –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä—ã?\n"
        "‚Ä¢ –ß—Ç–æ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –ì–ö –†–§?\n"
        "‚Ä¢ –ö–∞–∫–æ–≤ –ø–æ—Ä—è–¥–æ–∫ —Å—É–¥–µ–±–Ω–æ–≥–æ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤–∞?\n\n"
        "–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å ‚¨áÔ∏è"
    )
    await message.answer(text, parse_mode="Markdown")


@dp.message()
async def handle_question(message: types.Message):
    question = message.text.strip()
    if not question:
        await message.answer("‚ùå –í–æ–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        return

    await message.answer("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ...")

    answer = assistant.ask(question)

    if len(answer) <= 4000:
        await message.answer(answer)
    else:
        parts = []
        current = ""

        for line in answer.split("\n"):
            if len(current) + len(line) < 4000:
                current += line + "\n"
            else:
                parts.append(current)
                current = line + "\n"

        parts.append(current)

        for i, part in enumerate(parts):
            prefix = "" if i == 0 else f"(–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ {i + 1})\n"
            await message.answer(prefix + part)


async def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s"
    )
    await dp.start_polling(bot)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        sys.exit(0)
